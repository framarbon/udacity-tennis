{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "You are welcome to use this coding environment to train your agent for the project.  Follow the instructions below to get started!\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "Run the next code cell to install a few packages.  This line will take a few minutes to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mtensorflow 1.7.1 has requirement numpy>=1.13.3, but you'll have numpy 1.12.1 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mipython 6.5.0 has requirement prompt-toolkit<2.0.0,>=1.0.15, but you'll have prompt-toolkit 3.0.19 which is incompatible.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip -q install ./python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment is already saved in the Workspace and can be accessed at the file path provided below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "env = UnityEnvironment(file_name=\"/data/Tennis_Linux_NoVis/Tennis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. State and Action Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.         -6.65278625 -1.5        -0.          0.\n",
      "  6.83172083  6.         -0.          0.        ]\n",
      "The state for the all agents looks like: [[ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.         -6.65278625 -1.5        -0.          0.\n",
      "   6.83172083  6.         -0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.         -6.4669857  -1.5         0.          0.\n",
      "  -6.83172083  6.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])\n",
    "print('The state for the all agents looks like:', states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. The RL agent \n",
    "\n",
    "The agent can be found in the file ddpg_agent.py, which implements a standard DDPG with some adjustments that can be seen in the hyperparameters setup. \n",
    "\n",
    "Something important to mention is that during the first episodes we perform random actions instead of following the actor. The reason of this is to pretrain the model with highly explorative data in order to ensure a good training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from workspace_utils import active_session, keep_awake\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "def run_agent(agent, noise=True, n_episodes=1000, random_episodes=1000, max_t=1000, print_every=100):\n",
    "    np.random.seed(agent.config.seed)\n",
    "    scores = deque(maxlen=print_every)\n",
    "    mean_score= []\n",
    "    for i_episode in keep_awake(range(1, n_episodes+1)):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]      # reset the environment    \n",
    "        states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "#         agent.reset()\n",
    "        score = np.zeros(2)\n",
    "        for t in range(max_t):\n",
    "            if i_episode < random_episodes:\n",
    "                actions = 2*(np.random.rand(2,2)-0.5)\n",
    "            else:\n",
    "                actions = agent.act(states, add_noise=noise)\n",
    "            env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "            next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "            rewards = env_info.rewards                         # get reward (for each agent)\n",
    "            dones = env_info.local_done                        # see if episode finished            \n",
    "            \n",
    "            agent.step(states, actions, rewards, next_states, dones)\n",
    "            states = next_states\n",
    "            score += rewards\n",
    "#             if sum(rewards)>0.0001:\n",
    "#                 print('\\nI hit the ball! {:.2f}'. format(max(rewards)))\n",
    "            if np.any(dones):\n",
    "                break \n",
    "        scores.append(max(score))\n",
    "        mean_score.append(max(score))\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores)), end=\"\")\n",
    "        agent.save(prefix=agent.alg+'_checkpoint')\n",
    "        if i_episode % print_every == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores)))\n",
    "        if np.mean(scores) > 0.5:\n",
    "            print('\\rProblem solved in {} episodes!\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores)))\n",
    "            agent.save(prefix=agent.alg+'_model')\n",
    "            return mean_score\n",
    "    return mean_score\n",
    "\n",
    "def plot_scores(scores):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "    plt.ylabel('Score')\n",
    "    plt.xlabel('Episode #')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Set up agent hyperparameters\n",
    "\n",
    "The default hyper parameters are set in the following config. In this case epsilon referes to the scale and decay rate of the OU noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities import DictX\n",
    "\n",
    "def default_config():\n",
    "    return DictX({'n_agents': 2, 'batch_size': 128, 'buffer_size': int(1e6), 'learn_every': 1, 'tau': 1e-3, \\\n",
    "            'gamma': 0.99, 'lr_actor': 1e-4, 'lr_critic': 1e-4, 'limit_gradients': True, 'seed': 0, \\\n",
    "            'bn_active': False, 'first_layer_size': 128, 'second_layer_size': 128, 'dropout_rate': 0.0, \\\n",
    "            'epsilon_start': 0.2, 'epsilon_decay': 0.999, 'epsilon_min': 0.001, 'sigma': 0.5, \\\n",
    "            'path': 'trained_models/', 'alpha': 0.6,'beta': 0.4,'beta_step': 0.002})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Train the agent and visualize reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 0.03\n",
      "Episode 200\tAverage Score: 0.01\n",
      "Episode 300\tAverage Score: 0.02\n",
      "Episode 400\tAverage Score: 0.03\n",
      "Episode 500\tAverage Score: 0.02\n",
      "Episode 600\tAverage Score: 0.03\n",
      "Episode 700\tAverage Score: 0.06\n",
      "Episode 800\tAverage Score: 0.09\n",
      "Episode 900\tAverage Score: 0.10\n",
      "Episode 1000\tAverage Score: 0.12\n",
      "Episode 1100\tAverage Score: 0.19\n",
      "Problem solved in 1143 episodes!\tAverage Score: 0.52\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9f8f10e198>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ddpg_agent import DDPG\n",
    "            \n",
    "config = default_config()\n",
    "config.batch_size = 1024\n",
    "config.bn_active = True\n",
    "config.epsilon_decay = 0.999\n",
    "config.epsilon_start = 0.2\n",
    "ddpg = DDPG(state_size, action_size, config)\n",
    "scores = run_agent(ddpg, n_episodes=5000, random_episodes = 500)\n",
    "plot_scores(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Future Work\n",
    "As a future work it could be interesting to try a natural extension of DDPG which would be the Twin Delayed DDPG (TD3) or the Multiple Agents DDPG (MADDPG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
